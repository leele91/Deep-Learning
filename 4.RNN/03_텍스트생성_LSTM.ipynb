{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_텍스트생성_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx_9-PS_sBld"
      },
      "source": [
        "# LSTM을 이용한 텍스트 생성\r\n",
        "- '경마장에 있는 말이 뛰고 있다'\r\n",
        "- '그의 말이 법이다'\r\n",
        "- '가는 말이 고와야 오는 말이 곱다'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAYWSrhrr5x4"
      },
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oP52OA6yIAu"
      },
      "source": [
        "seed = 2021\r\n",
        "np.random.seed(seed)\r\n",
        "tf.random.set_seed(seed)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDJ6pTxO6ooh"
      },
      "source": [
        "# 말뭉치 만들기\r\n",
        "text=\"\"\"경마장에 있는 말이 뛰고 있다\\n\r\n",
        "그의 말이 법이다\\n\r\n",
        "가는 말이 고와야 오는 말이 곱다\\n\"\"\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e21pPbhn6y96"
      },
      "source": [
        "# 단어 집합 생성\r\n",
        "t = Tokenizer()\r\n",
        "t.fit_on_texts([text])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dssttu5d6okk",
        "outputId": "3fc8185e-4b21-43a1-bfbe-1355896e6dc4"
      },
      "source": [
        "# 단어 집합 크기 설정\r\n",
        "vocab_size = len(t.word_index) + 1\r\n",
        "# 케라스 토크나이저의 정수 인코딩은 인덱스가 1부터 시작하지만,\r\n",
        "# 케라스 원-핫 인코딩에서 배열의 인덱스가 0부터 시작하기 때문에\r\n",
        "# 배열의 크기를 실제 단어 집합의 크기보다 +1로 생성해야함 \r\n",
        "print('단어 집합의 크기 : %d' % vocab_size)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 집합의 크기 : 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOZG-jqm759z",
        "outputId": "fb2dba71-1624-4b02-fa4a-ac3a884fee96"
      },
      "source": [
        "# 문장기준으로 토크날하는 것\r\n",
        "sequences = []\r\n",
        "for line in text.split('\\n'): # \\n(라인)을 기준으로 문장 토큰화\r\n",
        "    encoded = t.texts_to_sequences([line])[0]\r\n",
        "    for i in range(1, len(encoded)):\r\n",
        "        sequence = encoded[:i+1]\r\n",
        "        sequences.append(sequence)\r\n",
        "\r\n",
        "print('학습에 사용할 샘플의 개수: %d' % len(sequences))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학습에 사용할 샘플의 개수: 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30ShYyTu752q",
        "outputId": "bbaa10da-9d0a-409a-fa33-350567b19525"
      },
      "source": [
        "# 모든 샘플에서 길이가 가장 긴 샘플의 길이 출력\r\n",
        "max_len = max(len(s) for s in sequences) \r\n",
        "print('샘플의 최대 길이 :', max_len)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "샘플의 최대 길이 : 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2M7Zn6W75zz"
      },
      "source": [
        "# 전체 샘플의 길이를 6(가장 긴 샘플의 길이)으로 패딩\r\n",
        "# 'pre' 옵션을 주면 앞을 0으로 패딩\r\n",
        "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA3cfOhuAdc1"
      },
      "source": [
        "# 각 샘플의 마지막 단어를 레이블로 분리\r\n",
        "X = sequences[:,:-1]\r\n",
        "y = sequences[:,-1]\r\n",
        "# 리스트의 마지막 값을 제외하고 저장한 것은 X\r\n",
        "# 리스트의 마지막 값만 저장한 것은 y. 이는 레이블에 해당됨."
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OE4nFpjbA-T2"
      },
      "source": [
        "# 레이블 데이터 y에 대해서 원-핫 인코딩을 수행\r\n",
        "Y = to_categorical(y, num_classes=vocab_size)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RofqV5X1yJwc"
      },
      "source": [
        "### 모델 정의\r\n",
        "- Embedding (Word Embedding)\r\n",
        "- LSTM\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beZp2_sND-Iq"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lz6nFZ1GyucB",
        "outputId": "a93880e3-be6f-4729-990e-ca4754d8b391"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(Embedding(vocab_size, 5, input_length=max_len-1))\r\n",
        "model.add(LSTM(32))\r\n",
        "model.add(Dense(vocab_size, activation='softmax'))\r\n",
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 5, 5)              60        \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 32)                4864      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 12)                396       \n",
            "=================================================================\n",
            "Total params: 5,320\n",
            "Trainable params: 5,320\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wzrlibfzlOH"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\r\n",
        "              optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bp4Sr9eF0aU4"
      },
      "source": [
        "history = model.fit(X, Y, epochs=200, verbose=0)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEw8MsPFGLFy",
        "outputId": "427339a0-f54a-4289-f9d5-1730d5cfaf13"
      },
      "source": [
        "history.history['accuracy'][-1]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7272727489471436"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbzEduhLGWsn"
      },
      "source": [
        "### 모델 검증"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNUmV00jGVt3"
      },
      "source": [
        "# 모델 검증용 문장을 생성하는 함수\r\n",
        "def sentence_generation(model, t, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\r\n",
        "    init_word = current_word # 처음 들어온 단어도 마지막에 같이 출력하기위해 저장\r\n",
        "    sentence = ''\r\n",
        "    for _ in range(n): # n번 반복\r\n",
        "        encoded = t.texts_to_sequences([current_word])[0] # 현재 단어에 대한 정수 인코딩\r\n",
        "        encoded = pad_sequences([encoded], maxlen=5, padding='pre') # 데이터에 대한 패딩\r\n",
        "        # result = model.predict_classes(encoded, verbose=0)\r\n",
        "        result = np.argmax(model.predict(encoded), axis=-1)\r\n",
        "    # 입력한 X(현재 단어)에 대해서 Y를 예측하고 Y(예측한 단어)를 result에 저장.\r\n",
        "        for word, index in t.word_index.items(): \r\n",
        "            if index == result: # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면\r\n",
        "                break # 해당 단어가 예측 단어이므로 break\r\n",
        "        current_word = current_word + ' '  + word # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\r\n",
        "        sentence = sentence + ' ' + word # 예측 단어를 문장에 저장\r\n",
        "        \r\n",
        "    # for문이므로 이 행동을 다시 반복\r\n",
        "    sentence = init_word + sentence\r\n",
        "    return sentence"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GFTm84AaQFJ"
      },
      "source": [
        "### 모델 변화\r\n",
        "- Embedding Vector 갯수: [2, 4, 6, 8]\r\n",
        "- LSTM 노드 갯수 : [24, 32]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39XKf-Nba4kE"
      },
      "source": [
        "def execute_model(n_embed, n_lstm):\r\n",
        "    model = Sequential()\r\n",
        "    model.add(Embedding(vocab_size, n_embed, input_length=max_len-1))\r\n",
        "    model.add(LSTM(n_lstm))\r\n",
        "    model.add(Dense(vocab_size, activation='softmax'))\r\n",
        "\r\n",
        "    model.compile(loss='categorical_crossentropy',\r\n",
        "                  optimizer='adam', metrics=['accuracy'])\r\n",
        "    \r\n",
        "    history = model.fit(X, Y, epochs=200, verbose=0)\r\n",
        "    print(f\"Accuracy: {history.history['accuracy'][-1]}\")\r\n",
        "    print(sentence_generation(model, t, '경마장에', 3))\r\n",
        "    print(sentence_generation(model, t, '그의', 2))\r\n",
        "    print(sentence_generation(model, t, '가는', 5))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIXYrMjJJYsg",
        "outputId": "138f0ca4-fadc-47e6-e6e6-af5e0c3634f9"
      },
      "source": [
        "for n_embed in [2, 4, 6, 8]:\r\n",
        "  for n_lstm in [24, 32]:\r\n",
        "    print(\"======================================\")\r\n",
        "    print(f'n_embed= {n_embed}, n_lstm= {n_lstm}')\r\n",
        "    execute_model(n_embed, n_lstm)\r\n",
        "    print()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======================================\n",
            "n_embed= 2, n_lstm= 24\n",
            "Accuracy: 0.4545454680919647\n",
            "경마장에 말이 말이 말이\n",
            "그의 말이 말이\n",
            "가는 말이 말이 말이 말이 뛰고\n",
            "\n",
            "======================================\n",
            "n_embed= 2, n_lstm= 32\n",
            "Accuracy: 0.5454545617103577\n",
            "경마장에 말이 말이 말이\n",
            "그의 말이 말이\n",
            "가는 말이 말이 말이 있다 곱다\n",
            "\n",
            "======================================\n",
            "n_embed= 4, n_lstm= 24\n",
            "Accuracy: 0.5454545617103577\n",
            "경마장에 말이 말이 뛰고\n",
            "그의 말이 말이\n",
            "가는 말이 말이 말이 뛰고 있다\n",
            "\n",
            "======================================\n",
            "n_embed= 4, n_lstm= 32\n",
            "Accuracy: 0.6363636255264282\n",
            "경마장에 말이 말이 뛰고\n",
            "그의 말이 말이\n",
            "가는 말이 말이 말이 말이 곱다\n",
            "\n",
            "======================================\n",
            "n_embed= 6, n_lstm= 24\n",
            "Accuracy: 0.6363636255264282\n",
            "경마장에 말이 말이 뛰고\n",
            "그의 말이 말이\n",
            "가는 말이 말이 말이 있다 있다\n",
            "\n",
            "======================================\n",
            "n_embed= 6, n_lstm= 32\n",
            "Accuracy: 0.7272727489471436\n",
            "경마장에 말이 말이 뛰고\n",
            "그의 말이 말이\n",
            "가는 말이 말이 오는 말이 곱다\n",
            "\n",
            "======================================\n",
            "n_embed= 8, n_lstm= 24\n",
            "Accuracy: 0.7272727489471436\n",
            "경마장에 말이 말이 뛰고\n",
            "그의 말이 말이\n",
            "가는 말이 말이 오는 곱다 곱다\n",
            "\n",
            "======================================\n",
            "n_embed= 8, n_lstm= 32\n",
            "Accuracy: 0.6363636255264282\n",
            "경마장에 말이 말이 뛰고\n",
            "그의 말이 말이\n",
            "가는 말이 말이 오는 말이 곱다\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bp57-JHHcIJP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}